import pandas as pd
import numpy as np
from time import sleep
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys


def souper(page_source):
    soup = BeautifulSoup(page_source, 'html.parser')
    return soup
def soupfinder(ElemName, ElemClass):
    result = soup.find(name=ElemName, class_=ElemClass).text
    return result

regex = r'&keywords=data%20analyst&refresh=true'


#private data login, password
userEmail = "userEmail"
userPass = "userPassword"
keywords = "Data analyst"


#empty arrays for pandas

pageSources = []
companyNames = []
positions = []
locations = []
contractTypes = []
requirements = []




#setting up chrome
chrome_options = Options()
chrome_options.add_argument("--no-sandbox")
driver = Service("chromedriver.exe")
browser = webdriver.Chrome(service=driver,options=chrome_options)


#fetching website
browser.get("https://linkedin.com/login")
sleep(5)

#logging in
username = browser.find_element(By.ID,"username").send_keys(userEmail)

pword = browser.find_element(By.ID,"password").send_keys(userPass)


browser.find_element(By.ID,"password").send_keys(Keys.RETURN)



#searching for a job
browser.get("https://www.linkedin.com/jobs/")
sleep(3)
search_bars = browser.find_elements(By.CLASS_NAME,'jobs-search-box__text-input')
search_keywords = search_bars[0]
search_keywords.send_keys(keywords)
sleep(2)
search_keywords.send_keys(Keys.RETURN)




#list jobs
list_items = browser.find_elements(By.CLASS_NAME,"occludable-update")
# scrolls a single page:
print(list_items)
for job in list_items:
    # executes JavaScript to scroll the div into view
    browser.execute_script("arguments[0].scrollIntoView();", job)
    job.click()
    page_source = browser.page_source
    #getting url
    url = browser.current_url
    url = re.sub(r'&keywords=data%20analyst&refresh=true','',url)
    print("###BEGIN###")
    print(url)
    pageSources.append(url)
    
    #souping
    soup = souper(page_source)
    
    #job title
    position = soupfinder('h2','t-24 t-bold')
    print(position)
    positions.append(position)
    
    # company name
    compName = soupfinder('span',"jobs-unified-top-card__company-name")
    compName=compName.strip()
    compName=compName.strip()
    print(compName)
    companyNames.append(compName)
    
    # locations
    location = soupfinder('span', "jobs-unified-top-card__bullet")
    location = location.strip()
    print(location)
    locations.append(location)
    #employment type
    span_tag = soup.find(name="li",class_="jobs-unified-top-card__job-insight",)
    sleep(2)
    contract = span_tag.find("span").text
    contract = contract.strip()
    print(contract)
    contractTypes.append(contract)
    
    #demands
    requirements.append("WIP")
    
    sleep(5)
    
  
    



#testing
print(companyNames)
print(len(companyNames))
print(pageSources)
print(len(pageSources))
print(positions)
print(len(positions))
print(contractTypes)
print(len(contractTypes))
print(locations)
print(len(locations))
print(requirements)
print(len(requirements))


#making a pd dataframe
resultsdataframe = pd.DataFrame({"company name": companyNames,
                                 "link": pageSources,
                                "position":positions,
                                "contract type":contractTypes,
                                "location":locations,
                                "requirements":requirements})
resultsdataframe
